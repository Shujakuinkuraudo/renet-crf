{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-05-04T01:06:33.219821Z",
     "end_time": "2023-05-04T01:06:33.260845Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import ICEWS18\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn.models.re_net import RENet\n",
    "\n",
    "import wandb\n",
    "from easydict import EasyDict\n",
    "\n",
    "wandb.login(key=\"8df071c79082d7ec99e9da99802221c4edef7d8c\")\n",
    "\n",
    "CFG = EasyDict()\n",
    "CFG.project = \"temporal-knowledge-base-completion\"\n",
    "CFG.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "CFG.epochs = 21\n",
    "CFG.seq_len = 5\n",
    "CFG.model = \"Renet\"\n",
    "CFG.tags = \"Baseline\"\n",
    "CFG.batch_size = 2048\n",
    "CFG.hidden_channels = 100\n",
    "\n",
    "\n",
    "def wandb_init():\n",
    "    config = {k: v for k, v in CFG.items() if '__' not in k}\n",
    "    run = wandb.init(\n",
    "        project=CFG.project,\n",
    "        name=f\"{CFG.model}-epoch-{CFG.epochs}\",\n",
    "        tags=CFG.tags,\n",
    "        config=config,\n",
    "        save_code=True\n",
    "    )\n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "__file__ = \"./\"\n",
    "# Load the dataset and precompute history objects.\n",
    "path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'ICEWS18')\n",
    "train_dataset = ICEWS18(path, pre_transform=RENet.pre_transform(CFG.seq_len))\n",
    "test_dataset = ICEWS18(path, split='test')\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True, follow_batch=['h_sub', 'h_obj'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False, follow_batch=['h_sub', 'h_obj'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-04T01:06:36.145282Z",
     "end_time": "2023-05-04T01:06:36.200100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader._SingleProcessDataLoaderIter object at 0x0000028136815240>\n",
      "tensor([0, 0, 0,  ..., 3, 3, 4])\n",
      "tensor([4, 1, 2,  ..., 3, 3, 4]) tensor([   4,    5,    5,  ..., 2045, 2045, 2046]) tensor([   24,    26,    27,  ..., 10228, 10228, 10234])\n",
      "torch.Size([2048, 5, 100])\n"
     ]
    }
   ],
   "source": [
    "from torch_scatter import scatter_mean\n",
    "\n",
    "print(train_loader.__iter__())\n",
    "for data in train_loader:\n",
    "    print(data.h_obj_t)\n",
    "    batch_size, seq_len = data.sub.size(0), CFG.seq_len\n",
    "    h_sub_t = data.h_sub_t + data.h_sub_batch * seq_len\n",
    "    print(data.h_sub_t, data.h_sub_batch, h_sub_t)\n",
    "    ent = torch.Tensor(train_dataset.num_nodes, 100)\n",
    "    h_sub = scatter_mean(ent[data.h_sub], h_sub_t, dim=0, dim_size=batch_size * seq_len).view(batch_size, seq_len, -1)\n",
    "    print(h_sub.size())\n",
    "    h_obj_t = data.h_obj_t + data.h_obj_batch * seq_len\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-04T01:06:40.112471Z",
     "end_time": "2023-05-04T01:06:40.379651Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize model and optimizer.\n",
    "model = RENet(train_dataset.num_nodes, train_dataset.num_rels, hidden_channels=CFG.hidden_channels, seq_len=seq_len,\n",
    "              dropout=0.2, ).to(CFG.device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.00001)\n",
    "run = wandb_init()\n",
    "wandb.watch(model, log='all')\n",
    "Total_params = 0\n",
    "Trainable_params = 0\n",
    "for param in model.parameters():\n",
    "    mulValue = np.prod(param.size())\n",
    "    Total_params += mulValue\n",
    "    if param.requires_grad:\n",
    "        Trainable_params += mulValue\n",
    "wandb.log({\"Total params\": Total_params, \"Trainable params\": Trainable_params})\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    # Train model via multi-class classification against the corresponding\n",
    "    # object and subject entities.\n",
    "    for data in train_loader:\n",
    "        data = data.to(CFG.device)\n",
    "        optimizer.zero_grad()\n",
    "        log_prob_obj, log_prob_sub = model(data)\n",
    "        loss_obj = F.nll_loss(log_prob_obj, data.obj)\n",
    "        loss_sub = F.nll_loss(log_prob_sub, data.sub)\n",
    "        loss = loss_obj + loss_sub\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    # Compute Mean Reciprocal Rank (MRR) and Hits@1/3/10.\n",
    "    result = torch.tensor([0, 0, 0, 0], dtype=torch.float)\n",
    "    for data in loader:\n",
    "        data = data.to(CFG.device)\n",
    "        with torch.no_grad():\n",
    "            log_prob_obj, log_prob_sub = model(data)\n",
    "        result += model.test(log_prob_obj, data.obj) * data.obj.size(0)\n",
    "        result += model.test(log_prob_sub, data:.sub) *data.sub.size(0)\n",
    "    result = result / (2 * len(loader.dataset))\n",
    "    return result.tolist()  #%%"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-04T01:06:42.527511Z",
     "end_time": "2023-05-04T01:06:55.174033Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Hits@1</td><td>▁▄▅▆▆▇▇▇▇███████████</td></tr><tr><td>Hits@10</td><td>▁▄▅▆▇▇▇▇████████████</td></tr><tr><td>Hits@3</td><td>▁▄▅▆▇▇▇▇████████████</td></tr><tr><td>MRR</td><td>▁▄▅▆▇▇▇▇████████████</td></tr><tr><td>Total params</td><td>▁</td></tr><tr><td>Trainable params</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Hits@1</td><td>0.1822</td></tr><tr><td>Hits@10</td><td>0.46237</td></tr><tr><td>Hits@3</td><td>0.31611</td></tr><tr><td>MRR</td><td>0.27751</td></tr><tr><td>Total params</td><td>16435966</td></tr><tr><td>Trainable params</td><td>16435966</td></tr><tr><td>epoch</td><td>20</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">Renet-epoch-21</strong> at: <a href='https://wandb.ai/shujakuin/temporal-knowledge-base-completion/runs/u3sxukop' target=\"_blank\">https://wandb.ai/shujakuin/temporal-knowledge-base-completion/runs/u3sxukop</a><br/>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 1 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20230504_010642-u3sxukop\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(1, 21):\n",
    "    wandb.log({\"epoch\": epoch})\n",
    "    train()\n",
    "    mrr, hits1, hits3, hits10 = test(test_loader)\n",
    "    wandb.log({\"MRR\": mrr, \"Hits@1\": hits1, \"Hits@3\": hits3, \"Hits@10\": hits10})\n",
    "torch.save(model.state_dict(), \"a.pt\")\n",
    "run.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-04T01:07:07.835362Z",
     "end_time": "2023-05-04T01:18:23.903033Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
