{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-09T10:00:11.164855600Z",
     "start_time": "2023-05-09T09:59:59.894586600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mshujakuin\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\zhouy/.netrc\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import ICEWS18\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn.models.re_net import RENet\n",
    "\n",
    "import wandb\n",
    "from easydict import EasyDict\n",
    "\n",
    "wandb.login(key=\"8df071c79082d7ec99e9da99802221c4edef7d8c\")\n",
    "\n",
    "CFG = EasyDict()\n",
    "CFG.project = \"temporal-knowledge-base-completion\"\n",
    "CFG.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "CFG.epochs = 21\n",
    "CFG.seq_len = 5\n",
    "CFG.model = \"Renet\"\n",
    "CFG.tags = \"Baseline\"\n",
    "CFG.batch_size = 2048\n",
    "CFG.hidden_channels = 100\n",
    "\n",
    "\n",
    "def wandb_init():\n",
    "    config = {k: v for k, v in CFG.items() if '__' not in k}\n",
    "    run = wandb.init(\n",
    "        project=CFG.project,\n",
    "        name=f\"{CFG.model}-epoch-{CFG.epochs}\",\n",
    "        tags=CFG.tags,\n",
    "        config=config,\n",
    "        save_code=True\n",
    "    )\n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\pythonProject\\cp310\\venv\\lib\\site-packages\\torch_geometric\\data\\dataset.py:190: UserWarning: The `pre_transform` argument differs from the one used in the pre-processed version of this dataset. If you want to make use of another pre-processing technique, make sure to delete 'data\\ICEWS18\\processed' first\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset and precompute history objects.\n",
    "path = osp.join(\"./\", 'data', 'ICEWS18')\n",
    "train_dataset = ICEWS18(path, pre_transform=RENet.pre_transform(CFG.seq_len))\n",
    "test_dataset = ICEWS18(path, split='test')\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True, follow_batch=['h_sub', 'h_obj'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False, follow_batch=['h_sub', 'h_obj'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T10:00:16.088165500Z",
     "start_time": "2023-05-09T10:00:15.926738900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader._SingleProcessDataLoaderIter object at 0x000001AE81FB9ED0>\n",
      "tensor([0, 0, 0,  ..., 4, 4, 4])\n",
      "tensor([0, 1, 1,  ..., 2, 3, 4]) tensor([   0,    0,    0,  ..., 2043, 2043, 2047]) tensor([    0,     1,     1,  ..., 10217, 10218, 10239])\n",
      "torch.Size([2048, 5, 100])\n"
     ]
    }
   ],
   "source": [
    "from torch_scatter import scatter_mean\n",
    "\n",
    "print(train_loader.__iter__())\n",
    "for data in train_loader:\n",
    "    print(data.h_obj_t)\n",
    "    batch_size, seq_len = data.sub.size(0), CFG.seq_len\n",
    "    h_sub_t = data.h_sub_t + data.h_sub_batch * seq_len\n",
    "    print(data.h_sub_t, data.h_sub_batch, h_sub_t)\n",
    "    ent = torch.Tensor(train_dataset.num_nodes, 100)\n",
    "    h_sub = scatter_mean(ent[data.h_sub], h_sub_t, dim=0, dim_size=batch_size * seq_len).view(batch_size, seq_len, -1)\n",
    "    print(h_sub.size())\n",
    "    h_obj_t = data.h_obj_t + data.h_obj_batch * seq_len\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T09:48:02.857101400Z",
     "start_time": "2023-05-09T09:48:02.567016100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize model and optimizer.\n",
    "model = RENet(train_dataset.num_nodes, train_dataset.num_rels, hidden_channels=CFG.hidden_channels, seq_len=seq_len,\n",
    "              dropout=0.2, ).to(CFG.device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.00001)\n",
    "run = wandb_init()\n",
    "wandb.watch(model, log='all')\n",
    "Total_params = 0\n",
    "Trainable_params = 0\n",
    "for param in model.parameters():\n",
    "    mulValue = np.prod(param.size())\n",
    "    Total_params += mulValue\n",
    "    if param.requires_grad:\n",
    "        Trainable_params += mulValue\n",
    "wandb.log({\"Total params\": Total_params, \"Trainable params\": Trainable_params})\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    # Train model via multi-class classification against the corresponding\n",
    "    # object and subject entities.\n",
    "    for data in train_loader:\n",
    "        data = data.to(CFG.device)\n",
    "        optimizer.zero_grad()\n",
    "        log_prob_obj, log_prob_sub = model(data)\n",
    "        loss_obj = F.nll_loss(log_prob_obj, data.obj)\n",
    "        loss_sub = F.nll_loss(log_prob_sub, data.sub)\n",
    "        loss = loss_obj + loss_sub\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    # Compute Mean Reciprocal Rank (MRR) and Hits@1/3/10.\n",
    "    result = torch.tensor([0, 0, 0, 0], dtype=torch.float)\n",
    "    for data in loader:\n",
    "        data = data.to(CFG.device)\n",
    "        with torch.no_grad():\n",
    "            log_prob_obj, log_prob_sub = model(data)\n",
    "        result += model.test(log_prob_obj, data.obj) * data.obj.size(0)\n",
    "        result += model.test(log_prob_sub, data:.sub) *data.sub.size(0)\n",
    "    result = result / (2 * len(loader.dataset))\n",
    "    return result.tolist()  #%%"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T10:00:26.138787900Z",
     "start_time": "2023-05-09T10:00:24.262298300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   2,    2,    2,  ..., 2047, 2047, 2047], device='cuda:0') tensor([ 415,  415, 3139,  ...,  458,   65,    1], device='cuda:0')\n",
      "tensor([1, 2, 3,  ..., 4, 4, 4], device='cuda:0') tensor([   2,    2,    2,  ..., 2047, 2047, 2047], device='cuda:0')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'h_sub_t' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m CFG\u001B[38;5;241m.\u001B[39mwandb:\n\u001B[0;32m      3\u001B[0m     wandb\u001B[38;5;241m.\u001B[39mlog({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepoch\u001B[39m\u001B[38;5;124m\"\u001B[39m: epoch})\n\u001B[1;32m----> 4\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m      6\u001B[0m mrr, hits1, hits3, hits10 \u001B[38;5;241m=\u001B[39m test(test_loader)\n",
      "Cell \u001B[1;32mIn[3], line 28\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m()\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(data\u001B[38;5;241m.\u001B[39mh_sub_batch, data\u001B[38;5;241m.\u001B[39mh_sub)\n\u001B[0;32m     27\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 28\u001B[0m log_prob_obj, log_prob_sub \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m     30\u001B[0m loss_obj \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mnll_loss(log_prob_obj, data\u001B[38;5;241m.\u001B[39mobj)\n",
      "File \u001B[1;32mF:\\pythonProject\\cp310\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mF:\\pythonProject\\cp310\\venv\\lib\\site-packages\\torch_geometric\\nn\\models\\re_net.py:182\u001B[0m, in \u001B[0;36mRENet.forward\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m    179\u001B[0m h_usub_t \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mh_sub_t \u001B[38;5;241m+\u001B[39m data\u001B[38;5;241m.\u001B[39mh_sub_batch \u001B[38;5;241m*\u001B[39m seq_len\n\u001B[0;32m    180\u001B[0m h_obj_t \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mh_obj_t \u001B[38;5;241m+\u001B[39m data\u001B[38;5;241m.\u001B[39mh_obj_batch \u001B[38;5;241m*\u001B[39m seq_len\n\u001B[1;32m--> 182\u001B[0m h_sub \u001B[38;5;241m=\u001B[39m scatter_mean(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39ment[data\u001B[38;5;241m.\u001B[39mh_sub], \u001B[43mh_sub_t\u001B[49m, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n\u001B[0;32m    183\u001B[0m                      dim_size\u001B[38;5;241m=\u001B[39mbatch_size \u001B[38;5;241m*\u001B[39m seq_len)\u001B[38;5;241m.\u001B[39mview(\n\u001B[0;32m    184\u001B[0m                          batch_size, seq_len, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    185\u001B[0m h_obj \u001B[38;5;241m=\u001B[39m scatter_mean(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39ment[data\u001B[38;5;241m.\u001B[39mh_obj], h_obj_t, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n\u001B[0;32m    186\u001B[0m                      dim_size\u001B[38;5;241m=\u001B[39mbatch_size \u001B[38;5;241m*\u001B[39m seq_len)\u001B[38;5;241m.\u001B[39mview(\n\u001B[0;32m    187\u001B[0m                          batch_size, seq_len, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    189\u001B[0m sub \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39ment[data\u001B[38;5;241m.\u001B[39msub]\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mrepeat(\u001B[38;5;241m1\u001B[39m, seq_len, \u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'h_sub_t' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 21):\n",
    "    wandb.log({\"epoch\": epoch})\n",
    "    train()\n",
    "    mrr, hits1, hits3, hits10 = test(test_loader)\n",
    "    wandb.log({\"MRR\": mrr, \"Hits@1\": hits1, \"Hits@3\": hits3, \"Hits@10\": hits10})\n",
    "torch.save(model.state_dict(), \"a.pt\")\n",
    "run.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T10:00:30.720148500Z",
     "start_time": "2023-05-09T10:00:29.254860700Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
